name: Daddylive Scraper

on:
  schedule:
    # Run every 6 hours
    - cron: '0 */6 * * *'
  workflow_dispatch: # Allow manual trigger

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.10'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests beautifulsoup4 python-dateutil urllib3
        
    - name: Create standalone scraper script
      run: |
        cat > scraper.py << 'SCRIPT'
        import requests
        from bs4 import BeautifulSoup
        from dateutil import parser
        from datetime import datetime, timedelta
        import json
        import re
        from urllib.parse import urlparse
        
        # Disable SSL warnings for this scraper
        import urllib3
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
        
        def extract_m3u8_from_iframe(iframe_url):
            """Extract m3u8 link from iframe source"""
            try:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36',
                    'Referer': iframe_url
                }
                
                response = requests.get(iframe_url, headers=headers, verify=False, timeout=15)
                text = response.text
                
                # Common patterns for m3u8 links
                m3u8_patterns = [
                    r'source:\s*["\']([^"\']*\.m3u8[^"\']*)["\']',
                    r'file:\s*["\']([^"\']*\.m3u8[^"\']*)["\']',
                    r'src:\s*["\']([^"\']*\.m3u8[^"\']*)["\']',
                    r'["\']([^"\']*\.m3u8[^"\']*)["\']',
                    r'(https?://[^\s<>"\']+\.m3u8[^\s<>"\']*)',
                ]
                
                for pattern in m3u8_patterns:
                    matches = re.findall(pattern, text)
                    if matches:
                        m3u8_url = matches[0]
                        # Clean up the URL
                        m3u8_url = m3u8_url.split('"')[0].split("'")[0]
                        
                        # Build proper headers for the m3u8
                        referer = iframe_url
                        origin = f"https://{urlparse(referer).netloc}"
                        
                        return {
                            'url': m3u8_url,
                            'headers': {
                                'Origin': origin,
                                'Referer': referer,
                                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36'
                            }
                        }
                
                return None
            except Exception as e:
                print(f"Error extracting m3u8 from {iframe_url}: {e}")
                return None
        
        def get_stream_link(stream_url, max_retries=2):
            """Get the playable m3u8 link from a stream page"""
            try:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36'
                }
                
                # Fetch the stream page
                response = requests.get(stream_url, headers=headers, verify=False, timeout=15)
                soup = BeautifulSoup(response.text, "html.parser")
                
                # Find iframe with id="thatframe" or any iframe
                iframe = soup.select_one("iframe#thatframe")
                if not iframe:
                    iframe = soup.find("iframe")
                
                if iframe and iframe.get("src"):
                    iframe_src = iframe.get("src")
                    
                    # Make absolute URL if relative
                    if iframe_src.startswith("//"):
                        iframe_src = "https:" + iframe_src
                    elif iframe_src.startswith("/"):
                        parsed = urlparse(stream_url)
                        iframe_src = f"{parsed.scheme}://{parsed.netloc}{iframe_src}"
                    
                    # Extract m3u8 from iframe
                    m3u8_data = extract_m3u8_from_iframe(iframe_src)
                    return m3u8_data
                
                return None
            except Exception as e:
                print(f"Error getting stream link from {stream_url}: {e}")
                return None
        
        def scrape_daddylive():
            domains = ["dlhd.dad"]
            timeout = 30
            items = []
            
            # Scrape schedule
            try:
                response = requests.get(f"https://{domains[0]}/schedule/schedule-generated.json", 
                                       timeout=timeout, 
                                       verify=False,
                                       headers={'User-Agent': 'Mozilla/5.0'})
                
                # Try to clean up potentially malformed JSON
                text = response.text.strip()
                
                # Check if it's wrapped in JavaScript
                if 'var ' in text or 'const ' in text or 'let ' in text:
                    # Extract JSON from JavaScript variable
                    match = re.search(r'[{[].*[}\]]', text, re.DOTALL)
                    if match:
                        text = match.group(0)
                
                # Try to parse
                try:
                    r = json.loads(text)
                except json.JSONDecodeError:
                    # If still failing, try to extract just the JSON object/array
                    start = text.find('{') if '{' in text else text.find('[')
                    end = text.rfind('}') if '}' in text else text.rfind(']')
                    if start != -1 and end != -1:
                        text = text[start:end+1]
                        r = json.loads(text)
                    else:
                        raise
                        
            except Exception as e:
                print(f"Error fetching schedule JSON: {e}")
                if 'response' in locals():
                    print(f"Status code: {response.status_code}")
                    print(f"Response text (first 500 chars): {response.text[:500]}")
                r = {}
            
            for header, events in r.items():
                for event_type, event_list in events.items():
                    for event in event_list:
                        title = event.get("event", "")
                        starttime = event.get("time", "")
                        league = event_type
                        channels = event.get("channels", [])
                        if isinstance(channels, dict):
                            channels = list(channels.values())
                        
                        try:
                            timestamp = parser.parse(header[:header.index("-")] + " " + starttime)
                            timestamp = timestamp.replace(year=2024)
                            utc_time = timestamp - timedelta(hours=1)
                        except:
                            try:
                                utc_time = datetime.now().replace(hour=int(starttime.split(":")[0]), minute=int(starttime.split(":")[1])) - timedelta(hours=1)
                            except:
                                utc_time = datetime.now()
                        
                        items.append({
                            "title": title,
                            "league": league,
                            "starttime": utc_time.isoformat(),
                            "links": [
                                {
                                    "address": f"https://{domains[0]}/stream/stream-{channel['channel_id']}.php",
                                    "name": channel["channel_name"]
                                } for channel in channels
                            ]
                        })
            
            # Scrape 24/7 channels from daddy.json
            try:
                r_channels = requests.get(f"https://{domains[0]}/daddy.json", 
                                         timeout=timeout,
                                         verify=False,
                                         headers={'User-Agent': 'Mozilla/5.0'})
                channels_data = r_channels.json()
                
                # daddy.json likely contains channel data in a structured format
                # Adjust parsing based on actual structure
                if isinstance(channels_data, list):
                    for channel in channels_data:
                        if isinstance(channel, dict):
                            title = channel.get('name', channel.get('title', 'Unknown'))
                            channel_id = channel.get('id', channel.get('channel_id', ''))
                            
                            if '18+' in title:
                                continue
                            
                            href = f"https://{domains[0]}/stream/stream-{channel_id}.php"
                            
                            items.append({
                                "title": title,
                                "league": "[COLORorange]24/7",
                                "starttime": None,
                                "links": [{
                                    "address": href,
                                    "name": title,
                                    "m3u8": None
                                }]
                            })
                elif isinstance(channels_data, dict):
                    # If it's a dict, iterate through values
                    for key, channel in channels_data.items():
                        if isinstance(channel, dict):
                            title = channel.get('name', channel.get('title', key))
                            channel_id = channel.get('id', channel.get('channel_id', key))
                            
                            if '18+' in title:
                                continue
                            
                            href = f"https://{domains[0]}/stream/stream-{channel_id}.php"
                            
                            items.append({
                                "title": title,
                                "league": "[COLORorange]24/7",
                                "starttime": None,
                                "links": [{
                                    "address": href,
                                    "name": title,
                                    "m3u8": None
                                }]
                            })
                        
            except Exception as e:
                print(f"Error fetching 24/7 channels from daddy.json: {e}")
                print(f"Falling back to scraping 24-7-channels.php")
                
                # Fallback to old method if daddy.json fails
                try:
                    r_channels = requests.get(f"https://{domains[0]}/24-7-channels.php", 
                                             timeout=timeout,
                                             verify=False,
                                             headers={'User-Agent': 'Mozilla/5.0'})
                    soup_channels = BeautifulSoup(r_channels.text, "html.parser")
                    A_link = soup_channels.find_all('a')[:2]
                    b_link = soup_channels.find_all('a')[8:]
                    links = A_link + b_link
                    
                    unique_hrefs = set()
                    for link in links:
                        title = link.text
                        if '18+' in title:
                            continue
                        
                        href = f"https://{domains[0]}{link['href']}"
                        if href in unique_hrefs:
                            continue
                        unique_hrefs.add(href)
                        
                        items.append({
                            "title": title,
                            "league": "[COLORorange]24/7",
                            "starttime": None,
                            "links": [{
                                "address": href,
                                "name": "",
                                "m3u8": None
                            }]
                        })
                except Exception as fallback_error:
                    print(f"Fallback also failed: {fallback_error}")
            
            return items
        
        if __name__ == "__main__":
            print("Starting Daddylive scraper...")
            items = scrape_daddylive()
            print(f"Scraped {len(items)} items")
            
            # Resolve m3u8 links for a sample of streams (to avoid too many requests)
            print("\nResolving m3u8 links for streams...")
            resolved_count = 0
            max_to_resolve = 50  # Limit to avoid rate limiting
            
            for item in items[:max_to_resolve]:
                for link in item.get('links', []):
                    stream_url = link.get('address')
                    if stream_url and '/stream/' in stream_url:
                        print(f"Resolving: {item['title']} - {link.get('name', 'Unknown')}")
                        m3u8_data = get_stream_link(stream_url)
                        if m3u8_data:
                            link['m3u8'] = m3u8_data
                            resolved_count += 1
                            print(f"  ✓ Found: {m3u8_data['url'][:80]}...")
                        else:
                            print(f"  ✗ No m3u8 found")
            
            print(f"\nResolved {resolved_count} m3u8 links")
            
            output = {
                "scraped_at": datetime.utcnow().isoformat() + "Z",
                "total_items": len(items),
                "resolved_m3u8_count": resolved_count,
                "items": items
            }
            
            with open('daddylive_data.json', 'w') as f:
                json.dump(output, f, indent=2)
            
            print(f"\nSuccessfully saved {len(items)} items to daddylive_data.json")
        SCRIPT
    
    - name: Run scraper
      run: python scraper.py
        
    - name: Commit and push if changes
      run: |
        git config --local user.email "github-actions[bot]@users.noreply.github.com"
        git config --local user.name "github-actions[bot]"
        git add daddylive_data.json
        git diff --quiet && git diff --staged --quiet || (git commit -m "Update Daddylive data - $(date -u +'%Y-%m-%d %H:%M:%S UTC')" && git push)
